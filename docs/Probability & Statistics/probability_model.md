---
layout: default
title: Probability Model
nav_order: 3
parent: Probability & Statistics
---

## Temas

- [Asignacion de Probabilidades](#asignaci√≥n-de-probabilidades-ley-de-laplace)
- [Axiomas de la Probabilidad](#axiomas-de-la-probabilidad)
 - [Regla de la Suma](#axioma-2-aditividad-regla-de-la-suma)
 - [Regla de multiplicaci√≥n](#axioma-3-regla-de-multiplicaci√≥n)
 - [Diagrama de Arbol](#el-diagrama-de-arbol)
- [Derivaciones de axiomas de probabilidad](#derivaciones-de-axiomas-de-probabilidad)
- [Analisis utilizando una Tabla de contingencia](#analisis-utilizando-una-tabla-de-contingencia)
- [La probabilidad condicional](#la-probabilidad-condicional)
- [Probabilidad condicional en la Ley de Probabilidad Total](#probabilidad-condicional-en-la-ley-de-probabilidad-total)
- [Independencia de los modelos de probabilidad](#independencia-de-los-modelos-de-probabilidad)
- [Modelos de probabilidad de Independencia condicional](#modelos-de-probabilidad-de-independencia-condicional)
- [Regla de Bayes](#regla-de-bayes)
- [Proyecto: crear un Clasificador de Bayes desde cero](#proyecto-crear-un-clasificador-de-bayes-desde-cero)

## Asignaci√≥n de Probabilidades: Ley de Laplace

### Modelo de Probabilidad

{: .note}
El modelo de probabilidad es un **marco conceptual y matem√°tico esencial en estad√≠stica y teor√≠a de probabilidad que permite analizar y predecir el comportamiento de fen√≥menos aleatorios.** Este modelo se basa en dos pilares fundamentales: **la definici√≥n del espacio muestral** que se aborda en la seccion anterior y **la asignaci√≥n de probabilidades** a los eventos dentro de este espacio.

El objetivo principal de un modelo de probabilidad es establecer una regla (la ley de probabilidad) **que asigne valores no negativos a los subconjuntos del espacio muestral, reflejando nuestra confianza o creencia en la ocurrencia de esos eventos**. Estos valores se conocen como probabilidades y representan nuestra estimaci√≥n de qu√© tan probable es que un resultado espec√≠fico, o un conjunto de resultados, ocurra. desarrolla esto

### Asignacion de Probabilidades

{: .highlight}
La asignaci√≥n de probabilidades es el **proceso mediante el cual se atribuyen valores num√©ricos a los eventos en el espacio muestral, reflejando la confianza o la creencia en la ocurrencia de esos eventos**. La probabilidad de un evento $$A$$, denotada como $$P(A)$$, es un **n√∫mero no negativo, entre 0 y 1**,  que indica qu√© tan probable es que $$A$$ ocurra cuando se realiza el experimento. 

### Ley de Laplace

{: .note}
Tambi√©n conocida como la **regla de la probabilidad cl√°sica** o equiprobable, establece que si un experimento aleatorio tiene resultados igualmente probables, entonces **la probabilidad de un evento $$ùê∏$$ es el n√∫mero de resultados en $$\|E\|$$ dividido por el n√∫mero total de resultados posibles en el espacio muestral $$\|S\|$$.**

$$P(E) = \frac{|E|}{|S|}$$

### Ejemplo Pr√°ctico

Imagina que un bol contiene 3 bolas rojas y 2 bolas azules. Si se va a extraer una bola al azar, la probabilidad de sacar una bola roja, usando la Ley de Laplace, se calcula como el n√∫mero de bolas rojas dividido por el n√∫mero total de bolas:

$$P(\text{Roja}) = \frac{3}{5}$$

Si no supi√©ramos los colores de las bolas en el bol, pero sabemos que hay 5 bolas, y asumimos equiprobabilidad (ignorando cualquier informaci√≥n adicional), la probabilidad de sacar cualquier bola espec√≠fica ser√≠a:

$$P(\text{Roja}) = \frac{1}{5}$$

## Axiomas de la Probabilidad

{: .highlight}
Son las reglas fundamentales **para definir las probabilidades de los eventos dentro del marco de la teor√≠a de probabilidad**. Estos axiomas son esenciales para construir modelos de probabilidad que permitan analizar y predecir el comportamiento de fen√≥menos aleatorios de manera matem√°ticamente rigurosa. Hay tres axiomas principales de probabilidad, y a partir de ellos se pueden derivar diversas reglas y propiedades.

### Axioma 1: No Negatividad

El primer axioma establece que **la probabilidad de cualquier evento es siempre un n√∫mero no negativo**. Esto refleja la idea de que la probabilidad mide la confianza o expectativa de que un evento ocurra, lo cual no puede ser una cantidad negativa. Este axioma tambi√©n permite que la probabilidad de un evento sea cero, lo que indica que el evento es imposible dentro del contexto del experimento.

### Axioma 2: Aditividad, Regla de la Suma

La **regla de la suma**, tambi√©n conocida como la **regla de adici√≥n**, es un principio fundamental en la teor√≠a de probabilidades que se utiliza para calcular la probabilidad de que ocurra al menos uno de dos eventos. 

**1 - Eventos Mutuamente Excluyentes**

Para dos eventos $$A$$ y $$B$$ que **no pueden ocurrir al mismo tiempo** (es decir, son mutuamente excluyentes), la regla de la suma establece que la probabilidad de que ocurra $$A$$ o $$B$$ es simplemente la suma de sus probabilidades individuales:

$$P(A \cup B) = P(A) + P(B)$$

Por ejemplo, si tienes un mazo de cartas y quieres saber la probabilidad de sacar un rey o una reina, sumar√≠as la probabilidad de sacar un rey con la probabilidad de sacar una reina, dado que no puedes sacar una carta que sea ambos a la vez.

**2 - Eventos No Mutuamente Excluyentes**

Cuando los eventos **pueden ocurrir simult√°neamente** (es decir, no son mutuamente excluyentes), se debe ajustar la regla de la suma para evitar contar dos veces la intersecci√≥n de los eventos. La regla modificada es:

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

Esto significa que se suma la probabilidad de $$A$$ y $$B$$, pero se resta la probabilidad de que $$A$$ y $$B$$ ocurran al mismo tiempo. Este ajuste es necesario para obtener una medida precisa de la probabilidad de que ocurra cualquiera de los eventos.

### Ejemplos Pr√°cticos de eventos no excluyentes

1. **Cartas**:
   - Si tienes un mazo de 52 cartas y quieres saber la probabilidad de sacar un as o un coraz√≥n, primero sumar√≠as la probabilidad de sacar un as $$4/52$$ con la probabilidad de sacar un coraz√≥n $$13/52$$. Luego, restar√≠as la probabilidad de sacar un as de corazones, la intersecci√≥n de los eventos $$1/52$$:
     - Sacar un As, 4 cartas: $$P(A) = \frac{4}{52}$$
     - Sacar un corazon (5, 6): $$P(B) = \frac{13}{52}$$
     - Sacar el As de corazones, 1 carta: $$P(A \cap B) = \frac{1}{52}$$
   
     $$
     P(\text{As} \cup \text{Coraz√≥n}) = \frac{4}{52} + \frac{13}{52} - \frac{1}{52} = \frac{16}{52} = \frac{4}{13}
     $$

     Los datos puedes darse de tres maneras: $$\frac{4}{13}$$, 0,3076 o como porcentaje multiplicando el resultado anterior por 100: 30,76%. 

2. **Dados**:
   - Si lanzas un dado y quieres saber la probabilidad de obtener un n√∫mero impar o un n√∫mero mayor que 4, primero determinar√≠as la probabilidad de cada uno de estos eventos y luego su intersecci√≥n:
     - N√∫mero impar (1, 3, 5): $$P(A) = \frac{3}{6}$$
     - N√∫mero mayor que 4 (5, 6): $$P(B) = \frac{2}{6}$$
     - N√∫mero impar y mayor que 4 (5): $$P(A \cap B) = \frac{1}{6}$$
   
     Aplicando la regla de la suma:
   
    $$P(A \cup B) = \frac{3}{6} + \frac{2}{6} - \frac{1}{6} = \frac{4}{6} = \frac{2}{3}$$

3. **Sala de espera**
   - En la sala de espera de un consultorio medico, el 60% de los pacientes toma cafe, el 50% lee revistas y unicamente el 20% delas personas toma cafe mientras lee revistas. Si se selecciona un paciente al azar ¬øcual es la probabilidad de que tome cafe o lea revistas?
   - C = pacientes que toman cafe
   - R = Pacientes que leen revistas
   - $$P(C \cup R) = 60\% + 50\% - 20\% = 90\%$$

    ![Ejercicio de Suma](https://fer78docs.github.io/assets/images/ejercicio3suma.jpg)

4. **Solucion utilizando una Tabla**
   - En una ciudad el 60% de las personas tienen ojos negros, el 80% tienen cabello negro y el 50% tienen cabello negro y ojos negros. Si se selecciona una persona al azar, calcule la probabilidad que:
   1. No tenga los ojos negros.
   2. Tenga los ojos o cabello negro
   - O = Personas con ojos negros
   - C = Personas con cabellos negros
  
  ![Ejercicio de Suma](https://fer78docs.github.io/assets/images/Ejercicio_2_suma.jpg)

  Solucion a traves de una tabla de contingencia:

  |               | O Negros | O No Negros | Total |
  |---------------|----------|-------------|-------|
  | C Negros      | 50%      | 30%         | 80%   |
  | C No Negros   | 10%      | 10%         | 20%   |
  | Total         | 60%      | 40%         | 100%  |

  Probabilidad de que no tenga ojos negros: 40%  
  Personas con ojos o cabellos negros: 

  $$P(O\cup C) = P(O) + P(C) - P(O\cup C) = 60\% + 80\% - 50\% = 90\%$$


### Axioma 3: Regla de multiplicaci√≥n

{: .highlight}
La **Regla de Multiplicaci√≥n** en probabilidad es una herramienta fundamental para calcular la probabilidad de que ocurran dos o m√°s eventos independientes o dependientes sucesivamente. Esta regla se aplica en dos formas principales, dependiendo de si los eventos son independientes o dependientes.

### 1. Eventos Independientes

Cuando los eventos son **independientes**, la probabilidad de que todos ocurran es simplemente el producto de sus probabilidades individuales. Esto se debe a que la ocurrencia de un evento no afecta la probabilidad de ocurrencia de otro.

**F√≥rmula:**

$$P(A \cap B) = P(A) \times P(B)$$

Donde $$P(A \cap B)$$ es la probabilidad de que ocurran tanto $$A$$ como $$B$$, y $$P(A) $$ y $$P(B)$$ son las probabilidades de que ocurran $$A$$ y $$B$$, respectivamente.

**Ejemplo:**
Si la probabilidad de sacar una carta roja de una baraja est√°ndar es $$\frac{1}{2}$$ y la probabilidad de sacar un rey es $$\frac{1}{13}$$, entonces la probabilidad de sacar una carta que sea tanto roja como un rey (asumiendo que cada extracci√≥n es independiente, como volver a meter la carta en la baraja y barajar de nuevo) es:

$$P(\text{Rey Rojo}) = \frac{1}{2} \times \frac{1}{13} = \frac{1}{26}$$

### 2. Eventos Dependientes

Cuando los eventos son **dependientes**, la probabilidad de la conjunci√≥n de eventos depende de la ocurrencia de los eventos anteriores.

**F√≥rmula:**

$$P(A \cap B) = P(A) \times P(B \mid A)$$

Donde $$P(B \mid A)$$ es la probabilidad de que ocurra $$B$$ dado que $$A$$ ya ha ocurrido.

**Ejemplo:**

Si sacas una carta de una baraja de 52 cartas y quieres saber la probabilidad de que la primera carta sea un as y la segunda tambi√©n sea un as, utilizar√≠as la regla de multiplicaci√≥n para eventos dependientes:

$$
\begin{aligned}
P(\text{As en primera carta}) = \frac{4}{52} \\
P(\text{As en segunda carta} \mid \text{As en primera carta}) = \frac{3}{51} \\
P(\text{Dos Ases}) = \frac{4}{52} \times \frac{3}{51} 
\end{aligned}
$$

### Ejercicios Eventos dependientes

**Ejercicio 1**  
En una urna hay 5 canicas azules, 2 rojas y una verde. Si se sacan canicas consecutivas al azar sin remplazo, calcule la probabilidad que:
  - La primera sea azul y la segunda sea verde.
  - Las dos sean rojas
  
  Calcular la probabilidad de que la primera canica sea azul $$A$$ y la segunda sea verde $$V$$
  
  $$P(A‚à©V) = P(A) * P(V|A) = \frac{5}{8} * \frac{1}{7} = \frac{5}{56} = 0.089 = 8,9%$$

   la probabilidad de que las dos sean rojas $$R$$:

  $$P(R‚à©R) = P(R) * P(R|R) = \frac{2}{8} * \frac{1}{7} = \frac{2}{56} = 0.035 = 3,5%$$



**Ejercicio 2**: En una clase hay 10 ni√±os y 8 ni√±as, si se seleccionan 3 estudiantes al azar:
  - ¬øCual es la probabilidad de que todos sean ni√±os?
  - ¬øCual es la probabilidad de que todos sean ni√±as?

 Probabilidad de que todos sean ni√±os:

  $$
  \begin{aligned}
  P(N¬∫‚à©N¬∫‚à©N¬∫) = P(N¬∫)  *  P(N¬∫|N¬∫)  *  P(N¬∫|N¬∫‚à©N¬∫) \\
  P(N¬∫‚à©N¬∫‚à©N¬∫) = \frac{10}{18} * \frac{9}{17} * \frac{8}{16} \\
  P(N¬∫‚à©N¬∫‚à©N¬∫) = \frac{5}{34} = 0.147 = 14,7\% \\
  \end{aligned}
  $$

 Probabilidad de que todos sean ni√±as:

  $$
  \begin{aligned}
  P(N¬™‚à©N¬™‚à©N¬™) = P(N¬™)  *  P(N¬™|N¬™)  *  P(N¬™|N¬™‚à©N¬™) \\
  P(N¬™‚à©N¬™‚à©N¬™) = \frac{8}{18} * \frac{7}{17} * \frac{6}{16} \\
  P(N¬™‚à©N¬™‚à©N¬™) = \frac{336}{4896} = 0.068 = 6,8\% \\
  \end{aligned}
  $$
  
Una forma de visualizar todos los resultados posibles de un par de eventos es un diagrama de √°rbol .

**Ejercicio 3:**En un curso el 80% de los estudiantes aprueba matematicas, el 75% aprueba ingles. De los estudiantes que apueban matematicas, el 90% tambien aprueban ingles. Si se selecciona un estudiante al azar y este ha aprovado ingles, que probabilidades hay de que haya aprovado matematicas?

  - Probabilidad de que aprueben matematicas: $$P(M) = 80\%$$
  - Probabilidad de que aprueben Ingles: $$P(I) = 75\%$$
  - Probabilidad de que aprueben Ingles dado que hayan aprobado matematicas: $$P(I\|M) = 90\%$$

  Para calcular la probabilidada de la interseccion, o sea de que haya aprobado las dos asignaturas, que se necesita para calcular el resultado 

  $$
  \begin{aligned}
  P(M \cap I) = P(M) * P(I|M) \\
  P(M \cap I) = 0.8 * 0.9 \\
  P(M \cap I) = 0.72 = 72\% \\
  \end{aligned} \\
  $$

Calcular el resultado del ejercicio. 

  $$
  \begin{aligned}
  P(M|I) = \frac{P(M \cap I)}{P(I)} \\
  P(M|I) = \frac{72\%}{75\%} \\
  P(M|I) = 0.96 = 96\% \\
  \end{aligned}
  $$

## El diagrama de arbol

{: .note}
Un **diagrama de √°rbol es una herramienta gr√°fica utilizada en estad√≠stica para representar todas las posibles consecuencias de una serie de decisiones o eventos sucesivos**. Este tipo de diagrama es √∫til para visualizar el espacio muestral de un proceso aleatorio, especialmente cuando este proceso implica varios pasos o decisiones secuenciales.

### Caracter√≠sticas de un Diagrama de √Årbol

1. **Nodos**: Puntos que representan los eventos o las decisiones. Los nodos iniciales se llaman nodos ra√≠z, y los nodos al final de cada rama se llaman nodos hoja.
2. **Ramas**: Conexiones entre nodos que representan el paso de un evento o decisi√≥n a otro. Cada rama est√° etiquetada con la probabilidad de pasar del nodo anterior al siguiente.

### C√≥mo Crear un Diagrama de √Årbol

#### Paso 1: Definir el Problema
Determina los eventos o decisiones que necesitas analizar. Establece claramente cu√°les son las opciones o posibles resultados en cada etapa.

#### Paso 2: Dibujar el Diagrama
- **Nodo Inicial**: Comienza con un nodo que represente el punto de partida del proceso o la primera decisi√≥n.
- **Ramas**: Desde el nodo inicial, dibuja ramas para cada posible resultado del evento o decisi√≥n. Etiqueta cada rama con la probabilidad correspondiente.
- **Repetir para Subsecuentes Decisiones o Eventos**: Para cada nuevo nodo creado en el paso anterior, repite el proceso de dibujar ramas para cada posible resultado siguiente.

#### Paso 3: Calcular Probabilidades
Utiliza las etiquetas de probabilidad en las ramas para calcular la probabilidad de secuencias de eventos. La probabilidad de cualquier secuencia de eventos es el producto de las probabilidades a lo largo del camino que representa esa secuencia en el diagrama.

### Ejemplo de un Diagrama de √Årbol: Lanzar dos monedas

Supongamos que lanzamos una moneda dos veces y queremos representar los posibles resultados.

- **Nodo Ra√≠z**: Comienza el diagrama.
- **Primer Lanzamiento**: Dos ramas, una para Cara (C) y otra para Cruz (Z), cada una con probabilidad de 0.5.
- **Segundo Lanzamiento**: Desde cada uno de los nodos resultantes del primer lanzamiento, dibuja otras dos ramas, una para Cara y otra para Cruz, tambi√©n cada una con probabilidad de 0.5.

![diagrama de arbol](https://fer78docs.github.io/assets/images/probability-tree-coin3.svg)

¬°En el navegador de la derecha podr√°s jugar con uno en este enlace: 

[Enlace al programa](https://static-assets.codecademy.com/skillpaths/master-stats-ii/intro-probability/tree-diagram/tree.html)



### Ejemplo online de evento independiente 

El subprograma del enlace hay un diagrama de √°rbol y diez canicas azules y naranjas debajo del diagrama de √°rbol (es posible que deba desplazarse hacia abajo para ver las canicas). Si haces clic en una de las canicas, el diagrama de √°rbol se completar√° seg√∫n lo que hayas seleccionado. 

Despu√©s de seleccionar dos canicas, aparecer√° una ecuaci√≥n para la regla del producto encima de las canicas. 

El n√∫mero de canicas naranjas y azules se aleatoriza cada vez que golpeas Reset, por lo que ver√°s muchos valores diferentes para prob1 , prob2 y final_prob .

Pruebe todos los resultados posibles y observe el diagrama de √°rbol en consecuencia:

[Enlace al programa](https://static-assets.codecademy.com/skillpaths/master-stats-ii/intro-probability/tree-diagram/tree.html)

### Axioma 3: Probabilidad del Espacio Muestral

El tercer axioma dicta que **la probabilidad del espacio muestral completo es 1.** Esto se alinea con la idea de que el espacio muestral representa todos los posibles resultados de un experimento, y dado que uno de esos resultados debe ocurrir, la certeza total se representa como una probabilidad de 1.

### Consecuencias de los Axiomas

A partir de estos axiomas, se pueden derivar varias propiedades importantes de la probabilidad:

- **Intervalo de Probabilidad:** Todo evento tiene una probabilidad que se encuentra en el intervalo $$[0, 1]$$, inclusivo. Esto se sigue del tercer axioma y del hecho de que ning√∫n evento puede ser m√°s probable que el espacio muestral completo.
- **Probabilidad del Complemento:** La probabilidad de que un evento no ocurra (su complemento) es 1 menos la probabilidad de que el evento ocurra.
- **Probabilidad del Conjunto Vac√≠o:** La probabilidad del conjunto vac√≠o, que representa un evento imposible, es 0. Esto es coherente con la interpretaci√≥n del conjunto vac√≠o como un evento sin resultados posibles.

## Derivaciones de axiomas de probabilidad

{: .highlight}
Las **derivaciones de los axiomas de probabilidad** se refieren a las propiedades y reglas que se pueden inferir directamente a partir de los axiomas fundamentales de la probabilidad. 

Estos axiomas, como ya se ha mencionado, son la no negatividad, la aditividad (o regla de suma para eventos disjuntos) y que la probabilidad del espacio muestral completo es 1. A partir de estos axiomas, se pueden deducir varias reglas importantes que son esenciales para el trabajo pr√°ctico en probabilidad y estad√≠stica.

### 1. Probabilidad del Complemento de un Evento

**Una de las derivaciones m√°s directas es la probabilidad del complemento de un evento**. Si $$A$$ es un evento, entonces el complemento de $$A$$, denotado $$A^c$$, representa la ocurrencia de $$no-A$$. Utilizando los axiomas, se puede demostrar que:

$$P(A^c) = 1 - P(A)$$

Esto se deduce del hecho de que $$A$$ y $$A^c$$ son mutuamente excluyentes y su uni√≥n es el espacio muestral completo, cuya probabilidad es 1.

### 2. Probabilidad de la Uni√≥n de Eventos

**Para dos eventos $$A$$ y $$B$$, la probabilidad de su uni√≥n puede ser expresada en t√©rminos de las probabilidades de $$A$$, $$B$$, y su intersecci√≥n $$A ‚à© B$$**. Esta regla se aplica incluso si $$A$$ y $$B$$ no son disjuntos y se deriva como sigue:

$$P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B)$$

Esto ajusta la aditividad para el caso de eventos qsue no son mutuamente excluyentes, evitando la sobrecontabilizaci√≥n de la intersecci√≥n de $$A$$ y $$B$$.

### 3. Subaditividad

La subaditividad se refiere a la propiedad de que **la probabilidad de la uni√≥n de cualquier colecci√≥n de eventos es menor o igual a la suma de sus probabilidades individuales.** Para una secuencia de eventos $$A1, A2,...., An$$:

Esta propiedad es particularmente √∫til para tratar con uniones de eventos que no son necesariamente disjuntos.

### 4. Probabilidad de Eventos Vac√≠os y Ciertos

Directamente de los axiomas, se establece que la **probabilidad del conjunto vac√≠o** $${‚àÖ}$$, que es un evento imposible, es 0:

$$P(‚àÖ) = 0$$

Asimismo, la probabilidad del espacio muestral completo $$(Œ©)$$, que representa un evento seguro, es 1:

$$P(Œ©) = 1$$

### 5. Monotonicidad

**Si un evento $$A$$ es un subconjunto de otro evento $$B$$, entonces la probabilidad de $$A$$ es menor o igual a la probabilidad de $$B$$**. Esto refleja la idea de que la ocurrencia de $$B$$ incluye la ocurrencia de $$A$$ junto con posiblemente otros resultados:

$$A ‚äÜ B -> P(A) ‚â§ P(B)$$

### 6. L√≠mites de Probabilidad

**Cualquier probabilidad $$P(A)$$ para un evento $$A$$ siempre estar√° en el rango de 0 a 1, inclusive.** Esto se deriva del hecho de que todas las probabilidades son no negativas y que la probabilidad del espacio muestral, el conjunto m√°s grande posible, es 1.

## Analisis utilizando una Tabla de contingencia

Una tabla de contingencia en probabilidad se utiliza para organizar y mostrar las probabilidades asociadas con combinaciones de categor√≠as de dos o m√°s variables aleatorias. Esta herramienta es muy √∫til para visualizar la distribuci√≥n conjunta de las variables, as√≠ como para calcular probabilidades condicionales y marginales.

### Creaci√≥n de una Tabla de Contingencia en Probabilidad

Para ilustrar c√≥mo se puede usar una tabla de contingencia en un contexto de probabilidad, consideremos un ejemplo sencillo con dos variables: tipo de veh√≠culo (auto o cami√≥n) y color (blanco o negro).

Supongamos que tenemos las siguientes probabilidades:

- La probabilidad de que un veh√≠culo sea un auto es 0.7 y un cami√≥n 0.3.
- Dada que es un auto, la probabilidad de que sea blanco es 0.4 y negro 0.6.
- Dada que es un cami√≥n, la probabilidad de que sea blanco es 0.5 y negro 0.5.

**Paso 1: Establecer las Probabilidades Conjuntas**

Calculamos las probabilidades conjuntas multiplicando la probabilidad de cada tipo de veh√≠culo por la probabilidad de cada color dado ese tipo. Por ejemplo, la probabilidad de que un veh√≠culo sea un auto blanco se calcular√≠a como $$0.7 \times 0.4 = 0.28$$.

#### Paso 2: Crear la Tabla

La tabla se organiza poniendo una variable en las filas y otra en las columnas, con las probabilidades calculadas en las celdas correspondientes.

| Veh√≠culo/Color | Blanco | Negro | Total |
|----------------|--------|-------|-------|
| Auto           | 0.28   | 0.42  | 0.7   |
| Cami√≥n         | 0.15   | 0.15  | 0.3   |
| **Total**      | 0.43   | 0.57  | 1     |

### An√°lisis de la Tabla

1. **Probabilidades Marginales**:
   - $$P(\text{Blanco}) = 0.43$$
   - $$P(\text{Negro}) = 0.57$$

2. **Probabilidades Condicionales**:
   - $$P(\text{Auto} \mid \text{Blanco}) = \frac{0.28}{0.43} \approx 0.651 $$
   - $$P(\text{Cami√≥n} \mid \text{Blanco}) = \frac{0.15}{0.43} \approx 0.349 $$

### Importancia y Usos

- **Visualizaci√≥n de Relaciones**: La tabla de contingencia muestra claramente c√≥mo se distribuyen las probabilidades entre diferentes categor√≠as de variables.
- **Facilita el C√°lculo**: Permite calcular f√°cilmente probabilidades marginales y condicionales.
- **An√°lisis Estad√≠stico**: Puede ser utilizada para realizar pruebas de independencia como el test de chi-cuadrado.

Tablas de contingencia como esta son extremadamente √∫tiles en estad√≠sticas, ciencia de datos, investigaci√≥n de mercados, y muchos otros campos donde las relaciones entre variables categ√≥ricas son importantes para el an√°lisis de decisiones y predicciones.


### Ejemplos de modelos de probabilidad discretos:

Este ejemplo nos permitir√° entender c√≥mo definir un espacio muestral y c√≥mo asignar probabilidades a los eventos dentro de ese espacio. Los modelos de probabilidad discretos se ocupan de experimentos donde el n√∫mero de posibles resultados es finito o contable. Un ejemplo cl√°sico es el lanzamiento de dados, donde los resultados posibles pueden listarse de manera expl√≠cita.

Supongamos que tenemos dos dados de cuatro caras, donde cada resultado tiene una probabilidad de $$1/16$$, podemos construir una tabla que muestre todas las combinaciones posibles de los resultados de los dos dados. Cada dado puede mostrar uno de cuatro resultados posibles (1, 2, 3, o 4), lo que nos da un total de $$4 \times 4 = 16$$ combinaciones posibles para los dos dados. Aqu√≠ est√° la tabla que modela el espacio muestral con la probabilidad asignada a cada par de resultados. Cada uno de estos nuemros deben ser positivos y la suma de todos debe ser 1.

|        | 1       | 2       | 3       | 4       |
|--------|---------|---------|---------|---------|
| **1**  | 1/16    | 1/16    | 1/16    | 1/16    |
| **2**  | 1/16    | 1/16    | 1/16    | 1/16    |
| **3**  | 1/16    | 1/16    | 1/16    | 1/16    |
| **4**  | 1/16    | 1/16    | 1/16    | 1/16    |

Cada celda representa una posibilidad de resultado: 

|        | 1       | 2       | 3       | 4       |
|--------|---------|---------|---------|---------|
| **1**  | {1,1}   | {1,2}   | {1,3}   | {1,4}   |
| **2**  | {2,1}   | {2,2}   | {2,3}   | {2,4}   |
| **3**  | {3,1}   | {3,2}   | {3,3}   | {3,4}   |
| **4**  | {4,1}   | {4,2}   | {4,3}   | {4,4}   |

Una vez establecido el espacio muestral y las probabilidades, podemos investigar ciertas preguntas:

#### Probabilidad de Suma Par 

¬øCu√°l es la probabilidad de que la suma de los resultados de los dos dados sea un n√∫mero par? Para responder, identificamos los pares cuya suma es par y calculamos la probabilidad combinada de estos pares.

|        | 1     | 2      | 3     | 4      |
|--------|-------|--------|-------|--------|
| **1**  | **2** | 3      | **4** | 5      |
| **2**  | 3     | **4**  | 5     | **6**  |
| **3**  | **4** | 5      | **6** | 7      |
| **4**  | 5     | **6**  | 7     | **8**  |

La cantidad de celdas con valores pares es 8 asi que seria 8 * 1/16 = 1/2, asi que la probabilidad de que la suma de los resultados sea un numero par es de un 50%.

#### Probabilidad de que al Menos un Dado Muestre un Cuatro:

¬øCu√°l es la probabilidad de que al menos uno de los dados muestre un cuatro? Nuevamente, seleccionamos los pares que cumplen con esta condici√≥n y sumamos sus probabilidades.
Segun nuestra tabla de espacio muestral hay 7 eventos que pueden tener 4 como resultado. Seria 7 * 1/16 = 7/16.

### Ejemplo Pr√°ctico: Predicciones de Enfermedades por un M√©dico

Imaginemos que tenemos un m√©dico con 20 a√±os de experiencia que, bas√°ndose en datos hist√≥ricos, ha desarrollado un modelo para predecir la probabilidad de que el pr√≥ximo paciente que llegue a su cl√≠nica tenga malaria o tifoidea, o ambas enfermedades. Seg√∫n este modelo:

- La probabilidad de que el pr√≥ximo paciente tenga malaria es del 0.6.
- La probabilidad de que el pr√≥ximo paciente tenga tifoidea es del 0.7.
- La probabilidad de que el pr√≥ximo paciente tenga tanto malaria como tifoidea es del 0.4.

Con esta informaci√≥n, se nos plantea la pregunta:   
**¬øCu√°l es la probabilidad de que el pr√≥ximo paciente no tenga ni malaria ni tifoidea?**

Para calcular la probabilidad de que el pr√≥ximo paciente no tenga ni malaria ni tifoidea, podemos utilizar la teor√≠a de probabilidad de conjuntos, espec√≠ficamente la ley de la probabilidad del complemento y la f√≥rmula para la uni√≥n de dos eventos.

### Leyes y F√≥rmulas Utilizadas

1. **Probabilidad del Complemento**:  
   $$P(\text{no } A) = 1 - P(A)$$
   Esta f√≥rmula se utiliza para encontrar la probabilidad de que un evento no ocurra, que es uno menos la probabilidad de que el evento ocurra.

2. **F√≥rmula de la Uni√≥n de Dos Eventos**:
   $$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$
   Aqu√≠, $$P(A \cup B)$$ representa la probabilidad de que ocurra al menos uno de los eventos $$A$$ o $$B$$, $$P(A)$$ y $$P(B)$$ son las probabilidades de los eventos individuales, y $$P(A \cap B)$$ es la probabilidad de que ambos eventos ocurran simult√°neamente.

Dado que:
- La probabilidad de tener Malaria: $$P(M) = 0.6 $$
- La probabilidad de tener Tifoidea:  $$P(T) = 0.7 $$
- La probabilidad de tener ambos: $$P(M \cap T) = 0.4$$

Queremos calcular $$P(\text{ni malaria ni tifoidea}) $$, que es el complemento de la probabilidad de que el paciente tenga malaria o tifoidea. Primero, calcularemos $$P(\text{malaria} \cup \text{tifoidea})$$ usando la f√≥rmula de la uni√≥n de dos eventos:

$$P(M \cup T) = P(M) + P(T) - P(M \cap T) = 0.6 + 0.7 - 0.4 = 0.9$$

Luego, aplicaremos la probabilidad del complemento para obtener la probabilidad de que el paciente no tenga ninguna de las dos enfermedades:

$$P(\text{ni malaria ni tifoidea}) = 1 - P(M \cup T) = 1 ‚àí 0.9 = 0.1$$

As√≠ que, basado en estos c√°lculos, la probabilidad de que el pr√≥ximo paciente no tenga ni malaria ni tifoidea es del 10%. Si puedes acceder a un entorno de Python, simplemente puedes utilizar estos c√°lculos directamente para confirmar el resultado.



## La probabilidad condicional

{: .note}
La probabilidad condicional nos indica la probabilidad de que ocurra un evento $$A$$, dado que ya sabemos que otro evento $$B$$ ha ocurrido. Esta se denota como $$P(A\|B)$$ y se calcula usando la f√≥rmula:

$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

donde $$P(A \cap B)$$ es la probabilidad de que ocurran $$A$$ y $$B$$ juntos, y $$P(B)$$ es la probabilidad de que ocurra $$B$$.

### Ejemplos Pr√°cticos 

1. **Ejemplo 1**
En una ciudad el 40% de la poblacion tiene pelo casta√±o (PC), el 25% tiene ojos casta√±os(OC) y el 15% tiene cabellos y ojos casta√±os. Si se selecciona una persona al azar y esta tiene ojos casta√±os ¬øCual es la probabilidad de que tenga tambien cabello casta√±o?
Formula de la probabilidad condicional:

$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

Solucion: 

- Probabilidad de que tenga el cabello casta√±o: $$P(C) = 40\%$$
- Probabilidad de que tenga los ojos casta√±os: $$P(O) = 25\% $$
- Probabilidad de que tenga el cabello casta√±o y los ojos casta√±os: $$P(C \cap O) = \15%$$

Formulacion: 


$$
\begin{aligned}
P(C|O) = \frac{P(C \cap O)}{P(O)} \\
P(C|O) = \frac{15}{25} \\
P(C|O) = \frac{0.15}{0.25} \\
P(C|O) = 0.6 = 60\%
\end{aligned}
$$


Resolviendo el ejercicio utilizando los diagramas de Venn, representamos la interseccion de los dos conjuntos, los de cabello casta√±o, los de ojos casta√±os. Como queremos saber la probabilidad de que al escojer una persona con los ojos casta√±os tenga tambien el cabello casta√±o, estaremos trabajando con el conjunto de las personas de ojos casta√±os y la interseccion con el conjunto de cabello casta√±o. 

![Ejercicio de Suma](https://fer78docs.github.io/assets/images/procondicional_ejercicio1.jpg)

$$
\begin{aligned}
P(C|O) = \frac{15}{25} \\
P(C|O) = \frac{0.15}{0.25} \\
P(C|O) = 0.6 = 60\%
\end{aligned}
$$



Imaginemos que tenemos un dado de seis caras que est√° cargado de tal manera que los n√∫meros pares tienen el doble de probabilidades de salir que los n√∫meros impares. Si definimos dos eventos:

- $$A$$: Sacar un n√∫mero no mayor que 4: $$A = \{1, 2, 3, 4\}$$.
- $$B$$: Sacar un n√∫mero par: $$B = \{2, 4, 6\}$$.

Dado que los n√∫meros pares tienen el doble de probabilidades de salir que los n√∫meros impares y que todos los n√∫meros pares entre s√≠ son igualmente probables, as√≠ como todos los n√∫meros impares entre s√≠, podemos establecer lo siguiente:

- Las probabilidades de sacar un n√∫mero par $$P(E2) = P(E4) = P(E6) = x$$ es el doble que la de sacar un n√∫mero impar $$P(E1) = P(E3) = P(E5) = y$$, por lo que $$x = 2y$$.
- Teniendo en cuenta que el dado es de seis caras y todas las probabilidades deben sumar 1, tenemos $$3x + 3y = 1$$.

Sustituyendo $$x$$ por $$2y$$ en la ecuaci√≥n $$3x + 3y = 1$$, obtenemos $$3(2y) + 3y = 1$$, lo que simplifica a $$6y + 3y = 1$$, dando $$9y = 1$$. De aqu√≠, podemos resolver $$y = \frac{1}{9}$$.

Como $$x = 2y$$, entonces $$x = \frac{2}{9}$$.

Esto significa que las probabilidades para cada resultado son:
- Para n√∫meros impares $$(1, 3, 5)$$: $$y = \frac{1}{9}$$.
- Para n√∫meros pares $$(2, 4, 6)$$: $$x = \frac{2}{9}$$.

Ahora, para calcular la probabilidad de $$P(A)$$ (sacar un n√∫mero no mayor que 4), sumamos las probabilidades de los resultados individuales en $$A$$:

$$
\begin{aligned}
P(A) = P(E1) + P(E2) + P(E3) + P(E4) \\
P(A) = \frac{1}{9} + \frac{2}{9} + \frac{1}{9} + \frac{2}{9} \\
P(A) = \frac{6}{9} \\
P(A) = \frac{2}{3}
\end{aligned}
$$

Por lo tanto, la probabilidad de sacar un n√∫mero no mayor que 4 con este dado cargado es de $$\frac{2}{3}$$.

**4. C√°lculo de la Probabilidad Condicional en el Ejemplo**

Para calcular $$P(A\|B)$$, primero identificamos la intersecci√≥n de $$A$$ y $$B$$, que es $$A ‚à© B = \{2, 4\}$$, cuyas probabilidades suman $$4/9$$. Normalizamos esta suma con la probabilidad de $$B$$, \(2/3\), obteniendo:

$$P(A|B) = \frac{4/9}{2/3} = \frac{4/9}{6/9} = \frac{4}{6} = \frac{2}{3}$$

Este resultado nos indica que, sabiendo que el resultado ser√° un n√∫mero par, la probabilidad de que tambi√©n sea un n√∫mero no mayor a 4 es $$2/3$$, igual que la probabilidad de $$A$$ sin condicionar. Esto demuestra la independencia de $$A$$ respecto a $$B$$.

## Probabilidad condicional en el aprendizaje autom√°tico

En el mundo del aprendizaje autom√°tico, la probabilidad condicional juega un papel crucial, siendo la base sobre la cual se construyen la mayor√≠a de los modelos.

#### Aplicaciones en el Aprendizaje Autom√°tico

El aprendizaje autom√°tico utiliza extensivamente la probabilidad condicional para modelar y predecir resultados bas√°ndose en datos hist√≥ricos o condiciones previas. Veamos algunos ejemplos:

- **Reconocimiento Facial:** Imagina una c√°mara que desbloquea una puerta si reconoce a una persona autorizada. Aqu√≠, la probabilidad condicional nos ayuda a predecir la probabilidad de que la persona frente a la c√°mara sea, de hecho, un empleado autorizado, bas√°ndose en la imagen capturada.

- **Reconocimiento de Actividades en Videos:** Al analizar un video, podemos querer identificar qu√© actividad se est√° realizando. Utilizamos variables aleatorias para representar tanto la actividad como los datos del video, y mediante la probabilidad condicional, modelamos la probabilidad de que se est√© realizando una actividad espec√≠fica.

- **Sistemas de Reconocimiento de Voz a Texto:** Aqu√≠, la probabilidad condicional se usa para modelar la probabilidad de que ciertas palabras o frases sean dichas, bas√°ndose en los sonidos capturados.

## Probabilidad condicional en la Ley de Probabilidad Total

La Ley de Probabilidad Total, un concepto fundamental en la teor√≠a de probabilidad que nos permite calcular la probabilidad de un evento a partir de la suma de las probabilidades de varios eventos mutuamente excluyentes que forman una partici√≥n del espacio muestral.


{: .note}
Para definir la **Ley de la Probabilidad Total**, Consideremos un espacio muestral $$\Omega$$ dividido en varios eventos mutuamente excluyentes $$A_1, A_2, ..., A_n$$ que forman una partici√≥n de $$\Omega$$. Esto significa que cada elemento de $$\Omega$$ pertenece exactamente a uno de los eventos $$A_i$$ y la uni√≥n de todos los $$A_i$$ recupera el espacio muestral completo. La Ley de Probabilidad Total nos dice que para cualquier evento $$B$$, la probabilidad de $$B$$ se puede expresar como:

$$
P(B) = P(B \cap A_1) + P(B \cap A_2) + ... + P(B \cap A_n)
$$

Para ilustrar, imagina que tenemos un espacio muestral representado por un conjunto $$\Omega$$, y este espacio se divide en 4 partes $$A_1, A_2, A_3$$ y $$A_4$$, formando una partici√≥n completa de $$\Omega$$. Supongamos que tenemos un evento $$B$$ que ocurre dentro de este espacio. La probabilidad de $$B$$ se puede calcular considerando sus intersecciones con cada uno de los conjuntos $$A_i$$.

![Ley de la probabilidad Total](https://fer78docs.github.io/assets/images/ley_probabilidad_total.jpg)

Esta ley es especialmente √∫til cuando trabajamos con variables aleatorias que representan datos reales. A menudo, nos encontramos con distribuciones conjuntas que involucran varias variables aleatorias y deseamos calcular distribuciones marginales. La Ley de Probabilidad Total nos proporciona una forma de hacerlo, sumando las probabilidades de eventos que est√°n condicionados por las particiones del espacio muestral.

## Independencia de los modelos de probabilidad

La **independencia**, tambi√©n conocida como independencia estad√≠stica, es una propiedad clave entre dos o m√°s eventos en la teor√≠a de la probabilidad. Dos eventos, $$A$$ y $$B$$, se consideran independientes si la ocurrencia de uno no afecta la probabilidad de ocurrencia del otro. Matem√°ticamente, esto se expresa como:

$$P(A \cap B) = P(A) \times P(B)$$

donde $$P(A \cap B)$$ es la probabilidad de que ambos eventos sucedan simult√°neamente, y $$P(A)$$ y $$P(B)$$ son las probabilidades de que cada evento suceda independientemente.

### Ejemplos de Independencia

1. **Lanzamiento de una moneda y un dado**: Supongamos que lanzamos una moneda y un dado al mismo tiempo. La probabilidad de que la moneda muestre cara $$P(cara) = \frac{1}{2}$$, y la probabilidad de que el dado muestre un seis $$P(B_6) = \frac{1}{6}$$. Estos dos eventos son independientes porque el resultado de la moneda no influye en el resultado del dado, entonces:

$$P(A \cap B) = P(A) \times P(B) = \frac{1}{2} \times \frac{1}{6} = \frac{1}{12}$$

2. **Elegir cartas de diferentes barajas**: Si eliges una carta de una baraja y otra de una baraja diferente, estos dos eventos son independientes porque el resultado de una elecci√≥n no afecta el resultado de la otra.

### Profundizaci√≥n en la Independencia Mutua

La pregunta central en la transcripci√≥n es si la independencia es mutua; es decir, si $$A$$ es independiente de $$B$$, ¬øimplica esto que $$B$$ tambi√©n es independiente de $$A$$? La respuesta es s√≠, porque la independencia entre dos eventos **es siempre mutua** dado que la relaci√≥n matem√°tica no cambia al invertir los roles.

### Independencia en Conjuntos de M√°s de Dos Eventos

Cuando tratamos con m√°s de dos eventos, establecer la independencia se vuelve m√°s complejo. Para que tres eventos, por ejemplo, $$A$$, $$B$$, y $$C$$, sean mutuamente independientes, deben cumplir todas las siguientes condiciones:

- $$P(A \cap B \cap C) = P(A) \times P(B) \times P(C)$$
- $$P(A \cap B) = P(A) \times P(B)$$ 
- $$P(A \cap C) = P(A) \times P(C)$$
- $$P(B \cap C) = P(B) \times P(C)$$

Estas condiciones garantizan que la independencia se mantiene en todos los subconjuntos de eventos.

## Modelos de probabilidad de Independencia condicional

{: .note}
La **independencia condicional** es un concepto m√°s refinado que indica que **dos eventos pueden ser independientes bajo la condici√≥n de un tercer evento**. Matem√°ticamente, dos eventos $$A$$ y $$B$$ son condicionalmente independientes dado un tercer evento $$C$$ si:

$$P(A \cap B | C) = P(A | C) \times P(B | C)$$

Este concepto es crucial en contextos donde la independencia absoluta no se mantiene, pero se establece una vez que se considera un evento condicionante.

### Ejemplo de Independencia Condicional

Considera el caso de testear un software bajo diferentes condiciones de red. Dos fallos distintos, $$A$$ y $$B$$, pueden no ser independientes en general (es decir, que uno ocurra puede afectar la probabilidad del otro), pero si condicionamos al tipo de red $$C$$ (por ejemplo, 4G vs. WiFi), estos fallos pueden volverse independientes dentro de cada tipo de red.

## Regla de Bayes

{: .note}
El **Teorema de Bayes**, nombrado as√≠ por el reverendo Thomas Bayes, es un principio fundamental en la teor√≠a de la probabilidad que **permite actualizar la probabilidad de un evento basado en nueva evidencia o informaci√≥n.** Este teorema es esencial para el razonamiento estad√≠stico y se utiliza en una amplia variedad de campos, desde la inteligencia artificial hasta la epidemiolog√≠a.

### F√≥rmula del Teorema de Bayes

La f√≥rmula del Teorema de Bayes relaciona las probabilidades condicionales e incondicionales de eventos estad√≠sticos. Se expresa matem√°ticamente como:

$$P(A \mid B) = \frac{P(B \mid A) \times P(A)}{P(B)}$$

Donde:
- $$P(A \mid B)$$ es la probabilidad posterior de $$A$$ dado $$B$$, o la probabilidad de $$A$$ despu√©s de haber observado $$B$$.
- $$P(B \mid A)$$ es la probabilidad de observar $$B$$ dado que $$A$$ es verdadero.
- $$P(A)$$ es la probabilidad a priori de $$A$$, o la probabilidad de $$A$$ antes de observar $$B$$.
- $$P(B)$$ es la probabilidad total de $$B$$, que se calcula utilizando la ley de probabilidad total.

### Entendiendo las Partes del Teorema

- **Probabilidad a Priori $$P(A)$$**: Es la probabilidad inicial de $$A$$ antes de que se tenga en cuenta cualquier informaci√≥n adicional.
- **Probabilidad Condicional $$P(B \mid A)$$**: Es la probabilidad de observar $$B$$ cuando $$A$$ es cierto.
- **Probabilidad a Posteriori $$P(A \mid B)$$**: Es la probabilidad revisada de $$A$$ despu√©s de tener en cuenta la evidencia $$B$$.
- **Probabilidad Marginal $$P(B)$$**: Tambi√©n conocida como la probabilidad total de $$ B$$, esta es la suma de las probabilidades de $$B$$ sobre todos los posibles resultados anteriores. Se calcula como:

  $$P(B) = P(B \mid A) \times P(A) + P(B \mid A^c) \times P(A^c)$$

  donde $$A^c$$ es el complemento de $$A$$.

### Ejemplo de Aplicaci√≥n

Para este problema, seguiremos el diagrama de √°rbol:

![Ley de la probabilidad Total](https://fer78docs.github.io/assets/images/ejercicio de bayes.png)

Supongamos que lo siguiente es cierto (esto se muestra en el primer conjunto de ramas del diagrama):

- El 20 por ciento de la poblaci√≥n tiene faringitis estreptoc√≥cica: $$P(ST) = 0.20$$
- El 80 por ciento de la poblaci√≥n no tiene faringitis estreptoc√≥cica: $$P(No ST) = 0.80$$

Ahora supongamos que hacemos pruebas a un grupo de personas para detectar faringitis estreptoc√≥cica. Los posibles resultados de estas pruebas se muestran en el siguiente conjunto de ramas:

- Si una persona tiene faringitis estreptoc√≥cica, hay un 85% de posibilidades de que su prueba sea positiva y un 15% de posibilidades de que sea negativa. Esto est√° etiquetado como:

 - Test Positivo: P(+\|ST) = 0.85
 - Test Negativo: P(-\|ST) = 0.15

Por otra parte: 

- Si una persona **NO** tiene faringitis estreptoc√≥cica, hay un 2% de posibilidades de que su prueba sea positiva y un 98% de posibilidades de que sea negativa. Esto est√° etiquetado como:

 - Test Positivo: P(+\|No ST) = 0.02
 - Test Negativo: P(-\|No ST) = 0.98

Finalmente, veamos los cuatro posibles pares de resultados que forman las ramas terminales de nuestro diagrama:

 - $$P(ST y + ) = 0.17$$ 
 - $$P(ST y - ) = 0.03$$ 
 - $$P(No ST y +) = 0.016$$ 
 - $$P(No ST y -) = 0.784$$  
‚Äã

En conjunto, estos suman uno, ya que capturan todos los resultados potenciales despu√©s de que se realizan las pruebas a los pacientes.

Es genial que tengamos toda esta informaci√≥n. Sin embargo, nos falta algo. Si alguien obtiene un resultado positivo, ¬øcu√°l es la probabilidad de que este correcto el test y realmente tenga faringitis estreptoc√≥cica? Notacionalmente, podemos escribir esta probabilidad como: $$P(ST\|+)$$

Imagine que es un paciente que recientemente dio positivo por faringitis estreptoc√≥cica. Es posible que desee saber la probabilidad de que TIENE faringitis estreptoc√≥cica, dado que su prueba dio positivo. Para calcular esta probabilidad, el Teorema de Bayes , que establece lo siguiente:

$$P(A \mid B) = \frac{P(B \mid A) \times P(A)}{P(B)}$$

Calculando

$$
\begin{aligned}
P(ST\|+) = \frac{P(+\|ST) \times P(ST)}{P(+)} \\
P(ST\|+) = \frac{0,85 \times 0,20}{P(+)} \\
\end{aligned}
$$

Qu√© pasa con P(+) ? ¬øEs esto algo que sabemos? Bueno, pensemos en esto. Hay cuatro resultados posibles:
‚Äã
- Tener faringitis estreptoc√≥cica y dar positivo
- Tener faringitis estreptoc√≥cica y dar negativo
- No tener faringitis estreptoc√≥cica y dar positivo
- No tener faringitis estreptoc√≥cica y dar negativo

Solo nos importan los dos resultados en los que un paciente da positivo en $$P(+)$$ . Por tanto, podemos decir:

$$
\begin{aligned}
P(+) = P(ST y +) + P(No ST y +) \\
P(+) = 0,17 + 0,16 \\
P(+) = 0,186 \\
\end{aligned}
$$

Continuamos con la formula principal:

$$
\begin{aligned}
P(ST\|+) = \frac{P(+\|ST) \times P(ST)}{P(+)} \\
P(ST\|+) = \frac{0,85 \times 0,20}{0,186} \\
P(ST\|+) = 0.914
\end{aligned}
$$

Existe un 91,4% de posibilidades de que realmente tenga faringitis estreptoc√≥cica si su prueba da positivo. Esto no es obvio a partir de la informaci√≥n descrita en nuestro diagrama de √°rbol, pero con el poder del teorema de Bayes, pudimos calcularlo.

Podemos calcular otras probabilidades condicionales como, $$P(ST\|-), P(NoST\|+), P(NoST\|-)$$.


### Aplicaciones en el Aprendizaje Autom√°tico

La Regla de Bayes es la base del clasificador Bayesiano, un algoritmo de aprendizaje autom√°tico que clasifica los datos bas√°ndose en la probabilidad de que pertenezcan a una clase dada la observaci√≥n de sus caracter√≠sticas. Este clasificador se utiliza en:
- Filtros de spam en correos electr√≥nicos.
- Clasificaci√≥n de documentos y textos.
- Diagn√≥sticos m√©dicos autom√°ticos.

### Modelo Generativo vs. Modelo Discriminativo

La distinci√≥n entre modelado generativo y discriminativo es fundamental en el aprendizaje autom√°tico:

- **Modelo Generativo**: Intenta modelar c√≥mo se generan los datos, combinando las distribuciones de las caracter√≠sticas y las clases. Ejemplos incluyen el clasificador Bayesiano y la mezcla de modelos Gaussianos.
- **Modelo Discriminativo**: Se enfoca en la frontera entre las clases, intentando directamente predecir la clase a partir de las caracter√≠sticas observadas. Ejemplos incluyen la regresi√≥n log√≠stica y las m√°quinas de soporte vectorial (SVM).


## Proyecto: crear un Clasificador de Bayes desde cero

Para el proyecto utilizaremos el conjunto de datos `iris` de la biblioteca `seaborn. El conjunto de datos Iris contiene mediciones de 150 flores de iris, espec√≠ficamente sus longitudes y anchuras de p√©talos y s√©palos. Cada flor es clasificada en una de tres especies: Setosa, Versicolor o Virginica. La carga de datos se realiza t√≠picamente con bibliotecas como Seaborn, que facilita la visualizaci√≥n y manipulaci√≥n de datos:

