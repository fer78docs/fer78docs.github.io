---
layout: default
title: Random Variables
nav_order: 4
parent: Probability & Statistics
---

## Temas 

- [Variables aleatorias discretas](#variables-aleatorias-discretas)
   - [Variable Aleatoria de Bernoulli](#variable-aleatoria-de-bernoulli)
- [Funci√≥n de Masa de Probabilidad (PMF)](#funci√≥n-de-masa-de-probabilidad-pmf)
- [Funci√≥n de distribuci√≥n acumulativa](#funci√≥n-de-distribuci√≥n-acumulativa)
- [Tipos de Variables Aleatorias Discretas](#tipos-de-variables-aleatorias-discretas)
- [Variables aleatorias continuas](#variables-aleatorias-continuas)
- [Funcion de densidad de probabilidad](#funcion-de-densidad-de-probabilidad)






## Variables Aleatorias

Las variables aleatorias son un concepto central en la teor√≠a de la probabilidad y estad√≠stica, representando cantidades que resultan de un experimento aleatorio y cuyos valores no son predecibles con certeza antes de realizar el experimento. Estas variables pueden clasificarse en dos grandes categor√≠as: variables aleatorias discretas y variables aleatorias continuas.

{: .highlight}
Una variable aleatoria puede ser entendida como una funci√≥n que asigna un valor num√©rico a cada resultado de un espacio muestral asociado con un experimento aleatorio. Por ejemplo, si lanzas un dado, el resultado (n√∫mero de puntos hacia arriba) es el valor que toma la variable aleatoria.

## Variables aleatorias discretas

{: .note}
Las **variables aleatorias discretas** son aquellas que toman un n√∫mero finito o contablemente infinito de valores distintos. Cada uno de estos valores tiene una probabilidad asociada, y la suma de todas estas probabilidades debe ser igual a uno. Este tipo de variable es muy com√∫n en la estad√≠stica y la teor√≠a de la probabilidad, y se utiliza frecuentemente para modelar situaciones donde los resultados son claramente separables y contables. Pueden adoptar valores que se pueden contar, como 0, 1, 2, ..., n, o una lista de valores espec√≠ficos como {-3, -1, 0, 1, 5}.

### Variable Aleatoria de Bernoulli

{: .highlight}
Una variable aleatoria de Bernoulli **es un tipo espec√≠fico de variable aleatoria discreta que s√≥lo toma dos posibles valores, t√≠picamente 0 y 1, para representar los resultados de un √∫nico ensayo de Bernoulli**. Este tipo de variable es fundamental en probabilidad y estad√≠stica para modelar el resultado de experimentos que tienen exactamente dos posibles resultados, com√∫nmente etiquetados como "√©xito" y "fracaso".

### Definici√≥n

Una variable de Bernoulli $$X$$ se define de la siguiente manera:
- $$X = 1$$ con probabilidad $$p$$ (√©xito).
- $$X = 0$$ con probabilidad $$1-p$$ (fracaso).


### Ejemplo con la Suma de Dos Dados

Si consideramos la variable $$S$$ como la suma de dos lanzamientos de un dado:
- $$P(S = 2) = \frac{1}{36}$$ porque solo hay una forma de obtener un 2: lanzando dos unos.
- $$P(S = 3) = \frac{2}{36}$$ porque hay dos combinaciones que dan un 3: (1,2) y (2,1).
- Y as√≠ sucesivamente hasta $$S = 12$$.

Estos c√°lculos asumen que todos los resultados son igualmente probables, lo cual es cierto para un dado justo.

### Aplicaci√≥n Pr√°ctica: Variable Aleatoria Bernoulli

{: .highlight}
Una **variable aleatoria Bernoulli** es un tipo especial de variable aleatoria discreta que solo toma dos valores, generalmente 0 y 1. Es extremadamente √∫til para modelar experimentos binarios como el lanzamiento de una moneda.

#### Ejemplo con el Lanzamiento de una Moneda
- Si la moneda es justa, $$P(X = 1) = 0.5$$ y $$P(X = 0) = 0.5$$.
- Si la moneda no es justa y la probabilidad de cara es 0.7, entonces $$P(X = 1) = 0.7$$ y $$P(X = 0) = 0.3$$.

La **PMF** en este caso asignar√≠a 0.7 a obtener cara y 0.3 a obtener cruz, reflejando la naturaleza sesgada de la moneda.

### Simulaci√≥n en Python

Vamos a construir una funci√≥n en Python que simule un experimento de Bernoulli, el cual podr√≠a representar, por ejemplo, el lanzamiento de una moneda:

```python
import numpy as np

def bernoulli_trial(p=0.5):
    """Simula un experimento de Bernoulli.
        Args:
    p (float): Probabilidad de √©xito (por defecto 0.5).
        Returns:
    int: 1 si el experimento resulta en √©xito, 0 en caso contrario.
    """
    return 1 if np.random.rand() <= p else 0
```

En esta funci√≥n, `np.random.rand()` genera un n√∫mero aleatorio entre 0 y 1, y compara este n√∫mero con la probabilidad de √©xito `p`. Si el n√∫mero generado es menor o igual a `p`, el resultado es un √©xito (1); de lo contrario, es un fracaso (0).

### Simulaci√≥n y An√°lisis de Resultados

Podemos usar esta funci√≥n para realizar m√∫ltiples ensayos y observar la frecuencia de √©xitos, lo que nos permite estimar la probabilidad de √©xito de la moneda:

```python
def simulate_bernoulli_trials(n, p=0.5):
    """Simula n ensayos de Bernoulli y reporta la frecuencia de √©xitos.
        Args:
    n (int): N√∫mero de ensayos.
    p (float): Probabilidad de √©xito.
    
    Returns:
    float: Frecuencia de √©xitos.
    """
    results = [bernoulli_trial(p) for _ in range(n)]
    return sum(results) / n

# Simular 1000 lanzamientos de una moneda con p = 0.7
n_trials = 1000
success_prob = 0.612199
frequency_of_success = simulate_bernoulli_trials(n_trials, success_prob)

print(f"La frecuencia de √©xito estimada es {frequency_of_success:.2f}")
```
Output:
```
La frecuencia de √©xito estimada es 0.71
```

Este c√≥digo simula 1000 lanzamientos de una moneda donde la probabilidad de obtener cara (√©xito) es del 70%. La funci√≥n simulate_bernoulli_trials devuelve la frecuencia de √©xitos, que deber√≠a acercarse a 0.7 a medida que el n√∫mero de ensayos aumenta.

### Visualizaci√≥n de la Convergencia

Para visualizar c√≥mo la frecuencia de √©xitos converge a la probabilidad real, podr√≠amos realizar m√∫ltiples simulaciones aumentando progresivamente el n√∫mero de ensayos y graficar los resultados:

```python
import matplotlib.pyplot as plt

trial_counts = [10, 50, 100, 500, 1000, 5000, 10000]
frequencies = [simulate_bernoulli_trials(count, success_prob) for count in trial_counts]

plt.figure(figsize=(10, 5))
plt.plot(trial_counts, frequencies, marker='o', linestyle='-')
plt.axhline(y=success_prob, color='r', linestyle='--')
plt.title('Convergencia de la Frecuencia de √âxitos a la Probabilidad Real')
plt.xlabel('N√∫mero de Ensayos')
plt.ylabel('Frecuencia de √âxitos')
plt.xscale('log')
plt.show()
```

![Visualizaci√≥n de la Convergencia](https://fer78docs.github.io/assets/images/Visualizaci√≥n de la Convergencia.png)

Este gr√°fico mostrar√° c√≥mo la frecuencia de √©xitos se estabiliza y converge hacia la probabilidad real de √©xito (0.612199 en este caso) a medida que aumenta el n√∫mero de ensayos, ilustrando la ley de los grandes n√∫meros.

## Funci√≥n de Masa de Probabilidad (PMF)  

{: .highlight}
La Funci√≥n de Masa de Probabilidad (PMF) es una funci√≥n que **describe la probabilidad de que una variable aleatoria discreta tome un valor espec√≠fico**. Es una funci√≥n que devuelve la probabilidad de que una variable aleatoria discreta sea exactamente igual a alg√∫n valor.‚Äã Es una funci√≥n que asocia a cada punto de su espacio muestral X la probabilidad de que esta lo asuma. La funci√≥n de probabilidad suele ser el medio principal para definir una distribuci√≥n de probabilidad discreta, y tales funciones existen para variables aleatorias escalares o multivariantes, cuyo dominio es discreto.

La funci√≥n de masa de probabilidad de un dado. Todos los n√∫meros tienen la misma probabilidad de aparecer cuando este es tirado.


![pmf_dado](https://fer78docs.github.io/assets/images/funcion_de_masa_probabilidad.png)

Por ejemplo, supongamos que lanzamos una moneda justa varias veces y contamos el n√∫mero de caras. La funci√≥n de masa de probabilidad que describe la probabilidad de cada resultado posible (p. ej., 0 caras, 1 cara, 2 caras, etc.) se denomina **distribuci√≥n binomial**. Los par√°metros para la distribuci√≥n binomial son:

- `n` para el n√∫mero de intentos (por ejemplo, `n=10` si lanzamos una moneda 10 veces)
- `p` para la probabilidad de √©xito en cada prueba (probabilidad de observar un resultado particular en cada prueba. En este ejemplo, `p= 0,5` porque la probabilidad de observar caras en un lanzamiento de moneda justo es 0,5)

Si lanzamos una moneda normal 10 veces, decimos que el n√∫mero de caras observadas sigue una distribuci√≥n `Binomial(n=10, p=0.5)`. El siguiente gr√°fico muestra la funci√≥n de masa de probabilidad para este experimento. Las alturas de las barras representan la probabilidad de observar cada resultado posible calculado por el PMF.

**Veamos c√≥mo cambia la forma de la distribuci√≥n binomial a medida que cambia el tama√±o de la muestra.**

Utilice el control deslizante para cambiar el valor de `x` lanzamientos de moneda justos, entre uno y diez. Las alturas de las barras resultantes representan la probabilidad de observar diferentes valores de caras en x n√∫mero de lanzamientos de moneda justos. Puede pasar el cursor sobre cada barra y ver el valor num√©rico real de la altura de la barra. Las barras m√°s altas representan resultados m√°s probables.

Observe que a medida que x aumenta, las barras se hacen m√°s peque√±as. Esto se debe a que la suma de las alturas de todas las barras siempre ser√° igual a 1. Entonces, cuando x es mayor, el n√∫mero de caras que podemos observar aumenta y la probabilidad debe dividirse entre m√°s valores.

[Binomial Distribution: Calculating Probability of a Given Number of Heads](https://static-assets.codecademy.com/skillpaths/master-stats-ii/probability-distributions/binomial/binomial_single.html)


### Calcular probabilidades usando Python

El m√©todo `binom.pmf()` de la biblioteca `scipy.stats` se puede utilizar para **calcular el PMF de la distribuci√≥n binomial en cualquier valor**. Este m√©todo toma 3 valores:

- `x`: el valor del inter√©s
- `n`: el n√∫mero de ensayos
- `p`: la probabilidad de √©xito

Por ejemplo, supongamos que lanzamos una moneda normal 10 veces y contamos el n√∫mero de caras. Podemos usar la funci√≥n `binom.pmf()` para calcular la probabilidad de observar 6 cabezas de la siguiente manera:

```python
# import necessary library
import scipy.stats as stats

# st.binom.pmf(x, n, p)
print(stats.binom.pmf(6, 10, 0.5))
```
Output
```
0.205078
```

Observe que dos de los tres valores que entran en el m√©todo `stats.binomial.pmf()` son los par√°metros que definen la distribuci√≥n binomial: `n` representa el n√∫mero de intentos y `p` representa la probabilidad de √©xito.

### Uso de la funci√≥n de masa de probabilidad en un rango

Hemos visto que podemos calcular la probabilidad de observar un valor espec√≠fico usando una funci√≥n de masa de probabilidad. ¬øQu√© pasa si queremos encontrar la probabilidad de observar un rango de valores para una variable aleatoria discreta? Una forma de hacer esto es sumando la probabilidad de cada valor.

Por ejemplo, digamos que lanzamos una moneda justa 5 veces y queremos saber la probabilidad de obtener entre 1 y 3 caras. Podemos visualizar este escenario con la funci√≥n de masa de probabilidad:

![Binomial Distribution: Calculating Probability of a Range](https://fer78docs.github.io/assets/images/Binomial-Distribution-PMF-Probability-over-a-Range.webp)

Podemos calcular esto usando la siguiente ecuaci√≥n donde `P(x)` es la probabilidad de observar el n√∫mero `x` de √©xitos (cara en este caso):

```
P(1 to 3 heads) = P(1<= X <=3)
P(1 to 3 heads) = P(X=1) + P(X=2) + P(X=3)
P(1 to 3 heads) = 0.1562 + 0.3125 + 0.3125
P(1 to 3 heads) = 0.7812
```

Visualicemos lo que significa tomar la probabilidad de un rango. Utilice los controles deslizantes para seleccionar un rango de valores que representen el n√∫mero de caras que podr√≠amos observar en 10 lanzamientos de moneda justos.

Pruebe diferentes rangos para ver c√≥mo cambian las probabilidades para diferentes valores. Pase el cursor sobre una barra individual para ver la altura de la barra (que corresponde a la probabilidad de que ocurra el valor).

‚Äã[Binomial Distribution: Calculating Probability of a Range](https://static-assets.codecademy.com/skillpaths/master-stats-ii/probability-distributions/binomial-range_v2/index.html)


### Funci√≥n de masa de probabilidad en un rango usando Python

Podemos utilizar el mismo m√©todo `binom.pmf()` de la biblioteca `scipy.stats` para calcular la probabilidad de observar un rango de valores. Como se mencion√≥ en un ejercicio anterior, el m√©todo `binom.pmf` toma 3 valores:

- `x`: el valor del inter√©s
- `n`: el n√∫mero de ensayos
- `p`: la probabilidad de √©xito

Por ejemplo, podemos calcular la probabilidad de observar entre 2 y 4 caras en 10 lanzamientos de moneda de la siguiente manera:
```python
import scipy.stats as stats

# calculating P(2-4 heads) = P(2 heads) + P(3 heads) + P(4 heads) for flipping a coin 10 times
print(stats.binom.pmf(2, n=10, p=.5) 
    + stats.binom.pmf(3, n=10, p=.5) 
    + stats.binom.pmf(4, n=10, p=.5))
```
Output:
```
0.366211
```

Tambi√©n podemos calcular la probabilidad de observar menos de un cierto valor, digamos 3 caras, sumando las probabilidades de los valores debajo de √©l:

```python
import scipy.stats as stats

# calculating P(less than 3 heads) = P(0 heads) + P(1 head) + P(2 heads) for flipping a coin 10 times
print(stats.binom.pmf(0, n=10, p=.5) 
    + stats.binom.pmf(1, n=10, p=.5) 
    + stats.binom.pmf(2, n=10, p=.5))
```
Output
```
0.0546875
```

Tenga en cuenta que debido a que nuestro rango deseado es inferior a 3 cabezas, no incluimos ese valor en la suma.

Cuando hay muchos valores de inter√©s posibles, esta tarea de sumar probabilidades puede resultar dif√≠cil. Si queremos saber la probabilidad de observar 8 o menos caras en 10 lanzamientos de moneda, debemos sumar los valores del 0 al 8:

```python
import scipy.stats as stats

var = stats.binom.pmf(0, n = 10, p = 0.5) 
    + stats.binom.pmf(1, n = 10, p = 0.5) 
    + stats.binom.pmf(2, n = 10, p = 0.5) 
    + stats.binom.pmf(3, n = 10, p = 0.5) 
    + stats.binom.pmf(4, n = 10, p = 0.5) 
    + stats.binom.pmf(5, n = 10, p = 0.5) 
    + stats.binom.pmf(6, n = 10, p = 0.5) 
    + stats.binom.pmf(7, n = 10, p = 0.5) 
    + stats.binom.pmf(8, n = 10, p = 0.5)
```
Output
```
0.98926
```
Esto implica una gran cantidad de c√≥digo repetitivo. En su lugar, tambi√©n podemos utilizar el hecho de que la suma de las probabilidades de todos los valores posibles es igual a 1:

```
P(0to8heads) + P(9to10heads) = P(0to10heads) = 1
P(0to8heads) = 1 ‚àí P(9to10heads)
```
Ahora, en lugar de sumar 9 valores para las probabilidades entre 0 y 8 caras, podemos hacer 1 menos la suma de dos valores y obtener el mismo resultado:

```python
import scipy.stats as stats
# less than or equal to 8
1 - (stats.binom.pmf(9, n=10, p=.5) + stats.binom.pmf(10, n=10, p=.5))
```
Output
```
0.98926
```

## Funci√≥n de distribuci√≥n acumulativa

{: .note}
La funci√≥n de distribuci√≥n acumulativa para una variable aleatoria discreta se puede derivar de la funci√≥n de masa de probabilidad. Sin embargo, en lugar de la probabilidad de observar un valor espec√≠fico, **la funci√≥n de distribuci√≥n acumulativa proporciona la probabilidad de observar un valor espec√≠fico O MENOS.**

Como se analiz√≥ anteriormente, las probabilidades de todos los valores posibles en una distribuci√≥n de probabilidad dada suman 1. **El valor de una funci√≥n de distribuci√≥n acumulativa en un valor dado es igual a la suma de las probabilidades menores que √©l, con un valor de 1 para la mayor n√∫mero posible.**


### CDF de Ejemplo para Diferentes Distribuciones
- **Distribuci√≥n Discreta**: Si una variable aleatoria $$ùëã$$ es discreta, la CDF tiene saltos en los puntos donde la variable tiene una probabilidad distinta de cero.
- **Distribuci√≥n Continua**: Si una variable aleatoria $$ùëã$$ es continua, la CDF es una funci√≥n continua. Ejemplo: para una distribuci√≥n normal con media $$ùúá$$ y desviaci√≥n est√°ndar $$ùúé$$, la CDF se representa usando la funci√≥n de error, la cual es una integral de la funci√≥n de densidad de probabilidad (PDF).

Mostramos c√≥mo se puede utilizar la funci√≥n de masa de probabilidad para calcular la probabilidad de observar menos de 3 caras en 10 lanzamientos de moneda sumando las probabilidades de observar 0, 1 y 2 caras. La funci√≥n de distribuci√≥n acumulativa produce la misma respuesta al evaluar la funci√≥n en CDF(X=2). En este caso, utilizar el CDF es m√°s sencillo que el PMF porque requiere un c√°lculo en lugar de tres.

La animaci√≥n del enlace muestra la relaci√≥n entre la `funci√≥n de masa de probabilidad` y la `funci√≥n de distribuci√≥n acumulativa`. El gr√°fico superior es el PMF, mientras que el gr√°fico inferior es el CDF correspondiente. Al observar la gr√°fica de una CDF, cada valor del eje y es la suma de las probabilidades menores o iguales que √©l en la PMF.

[Enlace a la animacion](https://static-assets.codecademy.com/skillpaths/master-stats-ii/probability-distributions/cdf-vs-pmf/animated.html)

Podemos usar una funci√≥n de distribuci√≥n acumulativa para calcular la probabilidad de un rango espec√≠fico tomando la diferencia entre dos valores de la funci√≥n de distribuci√≥n acumulativa. Por ejemplo, para encontrar la probabilidad de observar entre 3 y 6 caras, podemos tomar la probabilidad de observar 6 o menos cabezas y restar la probabilidad de observar 2 o menos caras. Esto deja un remanente de entre 3 y 6 cabezas.

La imagen de la derecha demuestra c√≥mo funciona esto. Es importante tener en cuenta que para incluir el l√≠mite inferior en el rango, el valor que se resta debe ser uno menos que el l√≠mite inferior. En este ejemplo, quer√≠amos saber la probabilidad de 3 a 6, que incluye 3. 

[Enlace a la animacion](https://static-assets.codecademy.com/skillpaths/master-stats-ii/probability-distributions/cdf-animation/animation.html)

### Usando la funci√≥n de distribuci√≥n acumulativa en Python

Podemos utilizar el m√©todo `binom.cdf()` de la biblioteca `scipy.stats` para calcular la funci√≥n de distribuci√≥n acumulativa. Este m√©todo toma 3 valores:

- `x`: el valor de inter√©s, buscando la probabilidad de este valor o menos
- `n`: el tama√±o de la muestra
- `p`: la probabilidad de √©xito

Calcular matem√°ticamente la probabilidad de observar 6 o menos caras en 10 lanzamientos de moneda justos (0 a 6 caras) se parece a lo siguiente:

```math
P(6 or fewer heads) = P(0 to 6 heads)
```
El codigo en Python es:
```python
import scipy.stats as stats

print(stats.binom.cdf(6, 10, 0.5))
```
Output
```
0.828125
```
Se puede pensar que calcular la probabilidad de observar entre 4 y 8 caras en 10 lanzamientos de moneda justos es tomar la diferencia del valor de la funci√≥n de distribuci√≥n acumulativa en 8 de la funci√≥n de distribuci√≥n acumulativa en 3:

```math
P(4 to 8 Heads) = P(0 to 8 Heads) ‚àí P(0 to 3 Heads)
```
En Python utilizamos el codigo:
```python
import scipy.stats as stats

print(stats.binom.cdf(8, 10, 0.5) - stats.binom.cdf(3, 10, 0.5))
```
Output
```
0.81738
```
Para calcular la probabilidad de observar m√°s de 6 caras en 10 lanzamientos de moneda justos, restamos el valor de la funci√≥n de distribuci√≥n acumulativa en 6 de 1. Matem√°ticamente, esto se parece a lo siguiente:
```math
P(more than 6 Heads) = 1 - P(6 or fewer Heads)
```
Tenga en cuenta que "m√°s de 6 cabezas" no incluye 6. En Python, calcular√≠amos esta probabilidad usando el siguiente c√≥digo:
```python
import scipy.stats as stats
print(1 - stats.binom.cdf(6, 10, 0.5))
```
Output
```
0.171875
```

## Tipos de Variables Aleatorias Discretas

Los tipos de variables aleatorias discretas se clasifican com√∫nmente seg√∫n las distribuciones de probabilidad que describen c√≥mo se comportan los datos asociados a estas variables. Aqu√≠ describo algunas de las distribuciones m√°s comunes y utilizadas para variables aleatorias discretas.
En Python, la librer√≠a `scipy.stats` proporciona implementaciones de las PMFs para muchas distribuciones discretas comunes, lo que facilita su c√°lculo en la pr√°ctica. Estas funciones son extremadamente √∫tiles para simulaciones, modelado estad√≠stico y an√°lisis probabil√≠stico en una amplia gama de aplicaciones.

1. **Distribuci√≥n Binomial**:  
   Usada cuando se est√°n observando el n√∫mero de √©xitos en un n√∫mero fijo de ensayos independientes y cada ensayo tiene solo dos posibles resultados (√©xito o fracaso).  
   La PMF de una distribuci√≥n binomial describe la probabilidad de obtener un n√∫mero espec√≠fico de √©xitos en un n√∫mero fijo de ensayos independientes, cada uno con dos posibles resultados y una misma probabilidad de √©xito.

    ```python
    from scipy.stats import binom
    import numpy

    # Par√°metros
    n = 10  # n√∫mero de ensayos
    p = 0.5  # probabilidad de √©xito
    size = 100 # muestras a generar

    # Generar Variables
    numpy.random.binomial(n, p, size)

    # Calcular PMF para un valor espec√≠fico
    k = 5  # n√∫mero de √©xitos
    prob = binom.pmf(k, n, p)
    print(f"Probabilidad de {k} √©xitos en {n} ensayos: {prob:.4f}")
    ```


2. **Distribuci√≥n de Poisson**:
    Aplica para modelar el n√∫mero de eventos en un intervalo de tiempo o espacio fijo cuando estos eventos ocurren con una tasa media conocida y de manera independiente entre s√≠.
    La PMF de una distribuci√≥n de Poisson mide la probabilidad de un n√∫mero determinado de eventos ocurriendo en un intervalo fijo, dado que estos eventos ocurren con una tasa media conocida y de manera independiente.

    ```python
    from scipy.stats import poisson
    import numpy

    lambda_ = 3  # tasa media de eventos por intervalo
    zize = 10 # n√∫mero de muestras a generar.

    # Generar variables
    numpy.random.poisson(lambda_, size)

    # Calcular PMF
    k = 4  # n√∫mero de eventos
    prob = poisson.pmf(k, lambda_)
    print(f"Probabilidad de {k} eventos: {prob:.4f}")
    ```


3. **Distribuci√≥n Geom√©trica**:
    Describe el n√∫mero de ensayos necesarios para obtener el primer √©xito en una secuencia de ensayos independientes, cada uno con dos posibles resultados.
    La PMF de una distribuci√≥n geom√©trica describe la probabilidad de que el primer √©xito ocurra en el ensayo $$ùëò$$

    ```python
    from scipy.stats import geom
    import numpy
    
    p = 0.2  # probabilidad de √©xito en cada ensayo
    zize = 10 # n√∫mero de muestras a generar.

    # generar variable
    numpy.random.geometric(p, size)

    # Calcular PMF
    k = 5  # ensayo en el que ocurre el primer √©xito
    prob = geom.pmf(k, p)
    print(f"Probabilidad de primer √©xito en el intento {k}: {prob:.4f}")
    ```

4. **Distribuci√≥n Binomial Negativa**:
    Generaliza la distribuci√≥n geom√©trica para contar el n√∫mero de ensayos requeridos para obtener un n√∫mero fijo de √©xitos.
    Esta distribuci√≥n cuenta cu√°ntos ensayos son necesarios para lograr un n√∫mero especificado de √©xitos.

    ```python
    from scipy.stats import nbinom
    import numpy

    r = 5  # n√∫mero de √©xitos deseados
    p = 0.5  # probabilidad de √©xito en cada ensayo
    zize = 10 # n√∫mero de muestras a generar.

    # generar variable
    numpy.random.negative_binomial(n, p, size)

    # Calcular PMF
    k = 10  # total de ensayos
    prob = nbinom.pmf(k-r, r, p)
    print(f"Probabilidad de alcanzar {r} √©xitos en {k} ensayos: {prob:.4f}")
    ```

5. **Distribuci√≥n Hipergeom√©trica**:
    Modela el n√∫mero de √©xitos en una muestra de tama√±o fijo tomada sin reemplazo de una poblaci√≥n que consta de dos tipos de objetos (√©xitos y fracasos).  
    La PMF de la distribuci√≥n hipergeom√©trica describe la probabilidad de obtener un n√∫mero determinado de √©xitos en una muestra extra√≠da sin reemplazo de una poblaci√≥n finita que consta de dos tipos de objetos.

    ```python
    from scipy.stats import hypergeom
    import numpy
    
    M = 20  # tama√±o total de la poblaci√≥n
    n = 7   # n√∫mero de √©xitos en la poblaci√≥n
    N = 12  # tama√±o de la muestra

    # Calcular PMF
    k = 3  # n√∫mero de √©xitos observados en la muestra
    prob = hypergeom.pmf(k, M, n, N)
    print(f"Probabilidad de {k} √©xitos en una muestra de {N}: {prob:.4f}")

    # generar variable
    ngood   # n√∫mero de elementos "buenos" en la poblaci√≥n.
    nbad    # n√∫mero de elementos "malos" en la poblaci√≥n.
    nsample # n√∫mero de elementos a muestrear (sin reemplazo).
    size    # n√∫mero de muestras a generar.

    numpy.random.hypergeometric(ngood, nbad, nsample, size=None)
   ```

6. **Distribuci√≥n Uniforme Discreta**:
   - Todos los valores posibles de la variable tienen la misma probabilidad de ocurrir.
   - Par√°metros: el m√≠nimo y m√°ximo valor que puede tomar la variable.

7. **Distribuci√≥n de Bernoulli**:
   - Un caso especial de la distribuci√≥n binomial con un solo ensayo $$n = 1$$.
   - Par√°metro: probabilidad de √©xito $$p$$.

8. **Distribuci√≥n Multinomial**:
   - Extensi√≥n de la distribuci√≥n binomial para situaciones en las que cada ensayo puede resultar en m√°s de dos categor√≠as.
   - Par√°metros: n√∫mero de ensayos $$n$$ y vector de probabilidades $$p$$ para cada categor√≠a.

Estas distribuciones permiten modelar una gran variedad de procesos y fen√≥menos en diversos campos como la biolog√≠a, ingenier√≠a, econom√≠a, ciencias sociales, y m√°s. Cada tipo de distribuci√≥n proporciona un modelo estad√≠stico que se adapta a las caracter√≠sticas espec√≠ficas de los datos y la naturaleza del experimento o la observaci√≥n realizada.


## Variables aleatorias continuas

{: .note}
Las **variables aleatorias continuas** son aquellas que pueden tomar cualquier valor num√©rico dentro de un intervalo o conjunto de intervalos, a diferencia de las variables aleatorias discretas que tienen valores contables y separados. Este tipo de variables es fundamental en estad√≠stica y probabilidad para modelar fen√≥menos que requieren una escala de medida infinita y continua.

### Caracter√≠sticas Principales

1. **Valores no Contables**: Las variables aleatorias continuas pueden adoptar cualquier valor dentro de un rango espec√≠fico, que puede ser finito o infinito.
2. **Funci√≥n de Densidad de Probabilidad (PDF)**: A diferencia de las variables discretas que usan una funci√≥n de probabilidad de masa, las continuas se describen mediante una funci√≥n de densidad de probabilidad. Esta funci√≥n no proporciona probabilidades directamente, sino que el √°rea bajo la curva de la funci√≥n entre dos puntos corresponde a la probabilidad de que la variable aleatoria caiga dentro de ese intervalo.

### Ejemplos de Variables Aleatorias Continuas

- **Altura de los estudiantes en una clase**: La altura puede variar continuamente y puede ser cualquier valor dentro de un rango razonable, por ejemplo, entre 1.50 metros y 2.00 metros.
- **Tiempo necesario para completar una tarea**: Este tiempo puede ser cualquier n√∫mero no negativo, medido con precisi√≥n hasta fracciones de segundo.
- **Presi√≥n en un tanque de gas**: La presi√≥n puede fluctuar y tomar cualquier valor dentro de los l√≠mites de seguridad del tanque.

## Funcion de densidad de probabilidad

De manera similar a c√≥mo las variables aleatorias discretas se relacionan con las funciones de masa de probabilidad, las **variables aleatorias continuas** se relacionan con las funciones de densidad de probabilidad. **Definen las distribuciones de probabilidad de variables aleatorias continuas y abarcan todos los valores posibles que puede adoptar la variable aleatoria dada.**

Cuando se representa gr√°ficamente, una funci√≥n de densidad de probabilidad es una curva que atraviesa todos los valores posibles que puede tomar la variable aleatoria, y el √°rea total bajo esta curva suma 1.

La siguiente imagen muestra una funci√≥n de densidad de probabilidad. El √°rea resaltada representa la probabilidad de observar un valor dentro del rango resaltado.

![Probability Density](https://fer78docs.github.io/assets/images/Adding-Area.gif)

En una funci√≥n de densidad de probabilidad, no podemos calcular la probabilidad en un solo punto. Esto se debe a que el √°rea de la curva debajo de un √∫nico punto es siempre cero. El siguiente gif muestra esto.

![Probability Density one point](https://fer78docs.github.io/assets/images/Normal-Distribution-Area-to-Zero.gif)

Como podemos ver en la imagen anterior, a medida que el intervalo se hace m√°s peque√±o, el ancho del √°rea bajo la curva tambi√©n se hace m√°s peque√±o. Al intentar evaluar el √°rea bajo la curva en un punto espec√≠fico, el ancho de esa √°rea se vuelve 0 y, por lo tanto, la probabilidad es igual a 0.

Podemos calcular el √°rea bajo la curva usando la funci√≥n de distribuci√≥n acumulativa para la distribuci√≥n de probabilidad dada.

Por ejemplo, las alturas caen bajo un tipo de distribuci√≥n de probabilidad llamada **distribuci√≥n normal**. Los par√°metros de la distribuci√≥n normal son la media y la desviaci√≥n est√°ndar, y utilizamos la forma *Normal(media, desviaci√≥n est√°ndar)* como abreviatura.

Sabemos que la altura de las mujeres tiene una media de 167,64 cm con una desviaci√≥n est√°ndar de 8 cm, lo que las sit√∫a bajo la distribuci√≥n Normal(167,64,8).

Digamos que queremos saber la probabilidad de que una mujer elegida al azar mida menos de 158 cm. Podemos usar la funci√≥n de distribuci√≥n acumulativa para calcular el √°rea bajo la curva de la funci√≥n de densidad de probabilidad de 0 a 158 para encontrar esa probabilidad.

![Area](https://static-assets.codecademy.com/skillpaths/master-stats-ii/probability-distributions/norm_pdf_167_8_filled.svg)

Podemos calcular el √°rea de la regi√≥n azul en Python usando el m√©todo `norm.cdf()` de la biblioteca `scipy.stats`. Este m√©todo toma 3 valores:

- `x`: el valor del inter√©s
- `loc:` la media de la distribuci√≥n de probabilidad
- `scale`: la desviaci√≥n est√°ndar de la distribuci√≥n de probabilidad

```python
import scipy.stats as stats

# stats.norm.cdf(x, loc, scale)
print(stats.norm.cdf(158, 167.64, 8))
```
Output
```
0.1141
```

## Funciones de densidad de probabilidad y funci√≥n de distribuci√≥n acumulativa

Podemos tomar la diferencia entre dos rangos superpuestos para calcular la probabilidad de que una selecci√≥n aleatoria est√© dentro de un rango de valores para distribuciones continuas. Este es esencialmente el mismo proceso que calcular la probabilidad de un rango de valores para distribuciones discretas.

![Rangos superpuestos](https://fer78docs.github.io/assets/images/Normal-PDF-Range.gif)

Digamos que queremos calcular la probabilidad de observar aleatoriamente a una mujer de entre 165 cm y 175 cm, suponiendo que las alturas todav√≠a siguen la distribuci√≥n Normal (167,74, 8). Podemos calcular la probabilidad de observar estos valores o menos. La diferencia entre estas dos probabilidades ser√° la probabilidad de observar aleatoriamente a una mujer en este rango dado. Esto se puede hacer en Python usando el m√©todo `norm.cdf()` de la biblioteca `scipy.stats`. Como se mencion√≥ anteriormente, este m√©todo adopta 3 valores:

```python
import scipy.stats as stats
# P(165 < X < 175) = P(X < 175) - P(X < 165)
# stats.norm.cdf(x, loc, scale) - stats.norm.cdf(x, loc, scale)
print(stats.norm.cdf(175, 167.74, 8) - stats.norm.cdf(165, 167.74, 8))
```
Output
```
0.45194
```

Tambi√©n podemos calcular la probabilidad de observar aleatoriamente un valor o mayor restando de 1 la probabilidad de observar menos que el valor dado. Esto es posible porque sabemos que el √°rea total bajo la curva es 1, por lo que la probabilidad de observar algo mayor que un valor es 1 menos la probabilidad de observar algo menor que el valor dado.

Digamos que queremos calcular la probabilidad de observar a una mujer que mide m√°s de 172 cent√≠metros, suponiendo que las alturas todav√≠a siguen la distribuci√≥n Normal (167,74, 8). Podemos pensar en esto como lo opuesto a observar a una mujer que mide menos de 172 cent√≠metros. Podemos visualizarlo de esta manera:

![Grafica](https://static-assets.codecademy.com/skillpaths/master-stats-ii/probability-distributions/norm_pdf_167_8_filled2.svg)

Podemos usar el siguiente c√≥digo para calcular el √°rea azul tomando 1 menos el √°rea roja:
```python
import scipy.stats as stats

# P(X > 172) = 1 - P(X < 172)
# 1 - stats.norm.cdf(x, loc, scale)
print(1 - stats.norm.cdf(172, 167.74, 8))
```
Output
```
0.45194
```


### Tipos de Variables Aleatorias Continuas

1. **Normal (Gaussiana)**:

   - `loc`: media de la distribuci√≥n (mu).
   - `scale`: desviaci√≥n est√°ndar de la distribuci√≥n (sigma).
   - `size`: n√∫mero de muestras a generar.

   ```python
   numpy.random.normal(loc=0.0, scale=1.0, size=None)
   ```
   Genera n√∫meros a partir de una distribuci√≥n normal con media `loc` y desviaci√≥n est√°ndar `scale`.

2. **Exponencial**:

   - `scale`: inverso de la tasa (lambda), a veces llamado par√°metro de escala.
   - `size`: n√∫mero de muestras a generar.

   ```python
   numpy.random.exponential(scale=1.0, size=None)
   ```
   Genera n√∫meros a partir de una distribuci√≥n exponencial con un par√°metro de escala.

3. **Uniforme**:

   - `low`: l√≠mite inferior del rango de los valores.
   - `high`: l√≠mite superior del rango de los valores.
   - `size`: n√∫mero de muestras a generar.

   ```python
   numpy.random.uniform(low=0.0, high=1.0, size=None)
   ```
   Genera n√∫meros a partir de una distribuci√≥n uniforme entre `low` y `high`.

4. **Beta**:

   - `a`: par√°metro de forma alpha.
   - `b`: par√°metro de forma beta.
   - `size`: n√∫mero de muestras a generar.

   ```python
   numpy.random.beta(a, b, size=None)
   ```
   Genera n√∫meros a partir de una distribuci√≥n beta con par√°metros `a` y `b`.

5. **Gamma**:

   - `shape`: par√°metro de forma (k).
   - `scale`: par√°metro de escala (theta).
   - `size`: n√∫mero de muestras a generar.

   ```python
   numpy.random.gamma(shape, scale=1.0, size=None)
   ```
   Genera n√∫meros a partir de una distribuci√≥n gamma con un par√°metro de forma y escala.

Estos m√©todos permiten simular datos que siguen estas distribuciones, lo que es √∫til en simulaciones, pruebas de hip√≥tesis, y para entender mejor el comportamiento estad√≠stico de fen√≥menos modelados por estas distribuciones.